{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from distribution_cost.configuration import spark_config\n",
    "from distribution_cost.configuration.app import AppConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:oracle:thin:BRC03_VMECA/8sUFYtvK@//pyox2k01:1521/BRCEX_PP2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database uri\n",
    "app_config = AppConfig()\n",
    "db_uri = app_config.db_uri_jdbc\n",
    "db_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark_context, spark_session = spark_config.get_spark(app_name=\"app-distribution-cost\",\n",
    "                                                      executors=4, executor_cores=4, executor_mem='4g',\n",
    "                                                      dynamic_allocation=True, max_executors=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_process(file_location):  \n",
    "  file_type = 'csv'\n",
    "  # CSV options\n",
    "  infer_schema = \"false\"\n",
    "  first_row_is_header = \"true\"\n",
    "  delimiter = \";\"\n",
    "\n",
    "  # The applied options are for CSV files. For other file types, these will be ignored.\n",
    "  df = spark_session.read.format(file_type) \\\n",
    "    .option(\"encoding\", 'ISO-8859-1') \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(file_location)\n",
    "\n",
    "  #Eliminate special characters in column names\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace('-', '')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace(' ', '_')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace('(', '_')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace(')', '_')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace('=', '_')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace('.', '_')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace('\\'', '_')) for c in df.columns))\n",
    "  df = df.select(*(F.col(\"`\" + c+ \"`\").alias(c.replace(';', '_')) for c in df.columns))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO_1= df_process(\"/user/e587247/data/landing/ope/Operations Commerciales - AP - client promotions.csv\")\n",
    "dfPROMO_2= df_process(\"/user/e587247/data/landing/ope/05-05-2020 - Network Remuneration - Base REF_PROMO - CSV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO = dfPROMO_1.union(dfPROMO_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO=dfPROMO.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_value_cat= {'Autres':'Other','Buy Back':'Buy Back','Hors scope':'Out of scope','Reprise':'Trade-in','Evénements':'Events','Services':'Services','Produit':'Product','Stock':'Stock','Remise':'Discount','CRM':'CRM','Financement':'Funding'}\n",
    "dfPROMO = dfPROMO.replace(to_replace=column_value_cat, subset=['Catégorie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO = dfPROMO.withColumn('Classification',F.when(F.col('Catégorie').isin('Discount','Events','Product','Funding','Trade-in','Services','Stock','CRM','Buy Back','Other','Out of scope'), 'Client Promotions').otherwise('Network Remuneration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO = dfPROMO.withColumn('Libellé_Opération_Commerciale',F.concat(F.col('Libellé_Opération_Commerciale'),F.lit(' - '), F.col('Classification')))\n",
    "dfPROMO=dfPROMO.drop('Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCat = spark_session.createDataFrame([('Unknown','Unknown')], dfPROMO.columns)\n",
    "dfPROMO = dfPROMO.union(newCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vinxpromo and do outer join on promo\n",
    "dfVINPROMO = spark_session.read.load(\"/user/brc03/vmeca/data/refined/vinpromo/COUNTRY=France\")\n",
    "dfVINPROMO = dfVINPROMO.withColumn('COUNTRY',F.lit('France')).drop('year','month','day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO = dfPROMO.withColumnRenamed('Libellé_Opération_Commerciale','LIB_OPC')\n",
    "dfPROMO = dfPROMO.withColumnRenamed('Catégorie','LIB_CAT')\n",
    "dfPROMO = dfPROMO.withColumn('COUNTRY',F.lit('France'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag promo as to be flagged (cat)\n",
    "# append to ref promo\n",
    "df_toFlagged= dfVINPROMO.filter(F.col('COUNTRY') == 'France').join(dfPROMO, on = ['LIB_OPC','COUNTRY'], how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toFlagged = df_toFlagged.select('LIB_OPC','LIB_CAT','COUNTRY').withColumn('LIB_CAT', F.lit('To be flagged'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO = dfPROMO.union(df_toFlagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO=dfPROMO.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load madax on oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO.write.option('truncate','true').jdbc(url=db_uri, table=\"SMKT005_REF_PROMO\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPROMO.write.mode(\"overwrite\").partitionBy(\"COUNTRY\").parquet(\"hdfs:///user/brc03/vmeca/data/refined/ope/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
